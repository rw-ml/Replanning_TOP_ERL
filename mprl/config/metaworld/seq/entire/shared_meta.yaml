# Global config
exp_name: &exp_name "metaworld_seq_entire"
exp_path: &exp_path "../../mprl_exp_result"
sub_exp_name: &sub_exp_name "Metaworld final"
act_func_hidden: &act_func_hidden leaky_relu
act_func_last: &act_func_last null
dtype: &dtype "float32"
device: &device "cuda"
seed: auto

# cw2 config
name: *exp_name
path: *exp_path
verbose_level: 1

# wandb
wandb:
  project: *exp_name
  group: *sub_exp_name
  entity: uiery
  log_interval: &log_interval 100
  log_model: true
  model_name: model

# experiment parameters
params:
  agent:
    type: TopErlAgentMultiProcessing
    # type: TopErlAgent
    args:
      lr_policy: 1e-3
      lr_critic: 5e-5
      wd_policy: 1e-5
      wd_critic: 1e-5
      use_mix_precision: true
      schedule_lr_policy: false
      schedule_lr_critic: false
      entropy_penalty_coef: 0.0
      discount_factor: 1
      epochs_policy: 15
      epochs_critic: 50
      balance_check: *log_interval
      evaluation_interval: *log_interval
      use_old_policy: true  # use a target policy as the old policy for the trust region projection
      old_policy_update_rate: 0.005  # update rate for the old policy from the current policy
      batch_size: 256
      critic_update_from: 2
      policy_update_from: 200
      dtype: *dtype
      device: *device

  mp: &mp
    type: prodmp
    args:
      num_dof: 4
      tau: 5
      alpha_phase: 3
      num_basis: 8
      basis_bandwidth_factor: 5
      num_basis_outside: 0
      alpha: 10
      disable_goal: false
      relative_goal: true
      auto_scale_basis: true
      weights_scale: 0.1
      goal_scale: 0.1
      dt: 0.0125
      dtype: *dtype
      device: *device

  policy:
    type: TopErlPolicy
    args:
      mean_net_args:
        avg_neuron: 128
        num_hidden: 2
        shape: 0.0
      variance_net_args:
        std_only: false
        contextual: false
      init_method: orthogonal
      out_layer_gain: 0.01
      min_std: 1e-5
      act_func_hidden: *act_func_hidden
      act_func_last: *act_func_last
      dtype: *dtype
      device: *device
      mp: *mp

  reference_split:
    correction_completion: "current_idx"
      #options: current_idx and "as_zero"
      #curr idx: every split gets time indexes fitting to the corresponding part of the trajectory
      #-> all individual basically start at end at the same spots, but vary in form [and previous steps all get cut out]--> parameters mean the same for all
      ############################################################
      # USE current_idx, as_zero CURRENTLY NOT FULLY IMPLEMENTED!#
      ############################################################
    #as_zero: every split will start at time index 0 --> each split starts at the current starting position [all steps after total step size get cut out]
    #-> goal parameter loses direct meaning as goal is not reachable in the used step size but would need last interval start+total_steps

    split_strategy: "n_equal_splits" #n_equal_splits, random_size_range, random_gauss, fixed_sizes, fixed_size_rand_start, intra_episode_fixed_inter_rand_size  _rand_semi_fixed_size_focus_on_region
    random_permute_splits: False #e.g. ranges = [12,15,36,0] -> permute that list every iteration before assignment, usable for ALL split_strategies

    n_splits: 3        #MOST IMPORTANT (for n_equal_splits): defines how many policy splits will be allowed, for rand variants defines how many splits will be saved (must be large enough to fit all splits, but should not be too large due to speed)
    #also required for split_strategy "n_equal_splits"
    ranges: [ 60, 40 ] #required for fixed_sizes --> number of steps assigned for each policy, len(ranged) = n_splits (else error)
    size_range: [ 15,30 ]  #required for split_strategy "random_size_range" -> random_size_range takes the splits from the sampler and so from the replay buffer
    # in the q and v update as splits, so that we dont introduce new gaps
    # will ensure that there are no splits < size_range[0] (except for first and last if next option==True)
    fixed_size: 50 #tune from [5,10,20,30,35] 5-35      #size for intermediate splits for fixed_size_rand_start

    use_top_erl_splits_policy: False #if true updates policy for random splits also with random_segments instead of from each predefined segment (does nothing for n equal splits)
    use_top_erl_splits_critic: True #analog to policy but for critic update
    policy_use_all_action_indexes: False #ONLY FOR TOP_ERL_UPDATES, if True -> add a smaller segment at the end of the random segments if it does not include action with last index

    ignore_top_erl_updates_after_index: 9999

    inter_fixed_size_range: [ 20, 35 ] #required for intra_episode_fixed_inter_rand_size
    #--> works like fixed_size_rand_start but for each trajectory fixed_size in sampled in between the two values
    focus_regions: [ 30, 80 ] #required for rand_semi_fixed_size_focus_on_region:  each value gives a separate region
      #[focus_region[i]-focus_regions_size_1way, focus_region[i]+focus_regions_size_1way],
    #around where the sampling size is decreased
    #-- uniformly sampled in [min_decreased_size, fixed_size] (include_last=True)
    focus_regions_size_1way: 15 #required for rand_semi_fixed_size_focus_on_region: (half-)size of focus region
    min_decreased_size: 10  #required for: rand_semi_fixed_size_focus_on_region:    min sampling distance in focus region
    size_exception_for_first_and_last_split: True #only for size_range -> default true so that v + q func will be computed at all timesteps
    mean_std: [ 10,5 ]    #required for split_strategy "random_gauss"

    q_loss_strategy: "overconfident" #"truncated" #q-loss is updated via time-slices --> this determines how the time-sliced get cut for the update
      #"truncated" --> one slice only for one parameter segment -> cut off parts that are not created with the same policy parameters
      #"start_unchanged" --> each slice has a startpoint from sampling saved in the replay buffer, with this sampling method
      # bigger gaps between two consecutive segments which use different parameters may occur, but the policies start with "known startpoints"
    #"continuing" --> take last reached point of previous policy as init point for next segment (parameters are set to fit a different point then)-> may also cause issues
    #enforce_no_overlap_overconf_include_0 -> enforces splits with include_0 also always updates first segment with 0 (before rand start)
    v_func_estimation: "overconfident" #"overconfident" #truncated, continuing or "overconfident"
    #overconfident: policy which is responsible for init time predicts entire sequence for v-func estimation


    re_use_rand_coord_from_sampler_for_updates: False #only for prodmp_reuse_sample
    add_d_state_to_critic: False #use extra d_state = decision state defining which state the parameters for the current policy are based on
    evaluate_on_equal_splits: False #if True --> all evaluations are done with the corresponding n_equal_splits n_splits

    include_time_in_states: True #default True -> else get rid of all time infos instates
    set_policy_update_init_cond_to_split_init: False #init conditions get mapped to last param initialization position -> true
    segments_equal_splits: False #update on non-random segments but ones that just equal the (policy-)splits

    policy_upd_init_cond_shift: 0
    policy_upd_init_cond_rel_shift_time: 0

  critic:
    type: TopErlCritic
    args:
      bias: true
      n_embd: 128
      block_size: 1024
      dropout: 0.0
      n_layer: 2
      n_head: 8
      update_rate: 0.005
      use_layer_norm: true
      relative_pos: false # false for abs pos encoding, true for relative pos encoding
      single_q: true
      dtype: *dtype
      device: *device

  projection:
    type: KLProjectionLayer  # KL-projection
    args:
      proj_type: kl
      mean_bound: 0.005
      cov_bound: 0.0005
      trust_region_coeff: 1.0
      scale_prec: true
      entropy_schedule: null  # Desired value is linear or exp or None
      target_entropy: 0.0 # target entropy per action dim
      temperature: 0.7
      entropy_eq: false # If the entropy should follow an equality constraint
      entropy_first: false # If the entropy should be the first constraint
      do_regression: false
      dtype: *dtype
      device: *device

  sampler:
    type: TopErlSampler
    args:
      env_id: "metaworld_ProDMP_TCE/pick-place-v2"
      traj_downsample_factor: 5  # Make the sequence shorter and faster for transformer
      dtype: *dtype
      device: *device
      seed: auto
      task_specified_metrics: [ "success"]

  replay_buffer:
    type: TopErlReplayBuffer
    args:
      buffer_size: 3000
      device: *device
      dtype: *dtype
